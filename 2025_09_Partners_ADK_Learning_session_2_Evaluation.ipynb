{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-ag/breakfast-with-adk/blob/main/2025_09_Partners_ADK_Learning_session_2_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Copyright 2025 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "```"
      ],
      "metadata": {
        "id": "TVOI6BovJQjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Second Session\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "cellView": "form",
        "id": "i1Ce7tbwJUp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Evaluate Your First AI Agent with the Google ADK\n",
        "\n",
        "Welcome! This notebook is your first step into the exciting world of **AI Agents**. An agent is more than just a chatbot; it's a smart program that uses a Large Language Model (LLM) like Gemini to **reason, plan, and use tools** to accomplish tasks.\n",
        "\n",
        "In this guide, we will build a simple \"Product Research Assistant\" agent. This agent will be able to answer questions about product details and prices by using specialized tools we provide it.\n",
        "\n",
        "We'll cover three key stages of professional agent development:\n",
        "1.  **üíª Implementation**: Defining the agent's logic and tools.\n",
        "2.  **üîß Unit Testing**: Verifying that each tool works correctly in isolation.\n",
        "3.  **üß™ Evaluation**: Testing the entire agent to see if it can reason correctly and choose the right tool for a given query.\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "intro_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### üéÅ üõë Important Prerequisite: Setup Your Environment! üõë üéÅ\n",
        "-----------------------------------------------------------------------------\n",
        "\n",
        "You will need a **Google AI API Key** to run this notebook.\n",
        "\n",
        "üëâ Follow the instructions [here](https://github.com/postak/colazione-con-adk/blob/main/Setting%20Up%20Your%20GCP%20Project%20%26%20Gemini%20API%20Key.pdf)"
      ],
      "metadata": {
        "id": "pM3HqQzFxecq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup: Installing Libraries\n",
        "\n",
        "First things first, we need to install the necessary Python libraries. We'll use the `pip` command to do this.\n",
        "\n",
        "* `google-adk`: This is the **Agent Development Kit (ADK)**. It provides the core building blocks for creating agents, like the `LlmAgent` and `FunctionTool` classes we'll use later.\n",
        "* `google-genai`: This library allows our Python code to communicate with the Google Gemini family of models, which will be the \"brain\" of our agent."
      ],
      "metadata": {
        "id": "install_explanation"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkXSpNgVSVeW"
      },
      "outputs": [],
      "source": [
        "# @title 1. Install Libraries\n",
        "!pip install google-adk google-genai -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Enter Your API Key\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# Securely get the API key from the user\n",
        "print(\"üîë Enter your Google AI API key to continue.\")\n",
        "google_api_key = getpass.getpass(\"   API key: \").strip()\n",
        "\n",
        "# Check if the key was provided and set it as an environment variable\n",
        "if not google_api_key:\n",
        "  raise ValueError(\"A Google AI API key is required to run this notebook.\")\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = google_api_key\n",
        "print(\"‚úÖ API key configured.\")"
      ],
      "metadata": {
        "id": "p8AJhl-GSliY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Implementation: Defining the Agent and Its Tools\n",
        "\n",
        "This is the core of our project! We'll define the agent's capabilities here. The `%%writefile agent.py` command is a special \"magic\" command in notebooks that saves the content of this cell into a new file named `agent.py`. This helps keep our code organized.\n",
        "\n",
        "Let's break down what's inside `agent.py`:\n",
        "\n",
        "### Agent Tools üõ†Ô∏è\n",
        "A **Tool** is a function that an agent can call to get information or perform an action. Here, we create two simple Python functions that act as our tools:\n",
        "- `get_product_details()`: Simulates looking up product information from a database.\n",
        "- `get_product_price()`: Simulates looking up a product's price.\n",
        "\n",
        "We then wrap these Python functions inside `FunctionTool(...)`. This ADK class inspects our function, understands its name and arguments (`product_name: str`), and makes it usable by the LLM.\n",
        "\n",
        "### The Agent's Brain üß†\n",
        "The `LlmAgent` is the central component. We configure it with:\n",
        "- **`model`**: We specify `gemini-2.5-flash`, a fast and powerful model perfect for this kind of task.\n",
        "- **`tools`**: We give the agent the list of tools it's allowed to use.\n",
        "- **`instruction`**: This is the most critical part! The instruction (or \"system prompt\") is a set of rules and guidelines that tells the agent how to behave. It's how we steer the LLM's reasoning. We explicitly tell it **when** to use each tool based on keywords in the user's query."
      ],
      "metadata": {
        "id": "agent_definition_explanation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Define the Agent and Tools (agent.py)\n",
        "%%writefile agent.py\n",
        "import textwrap\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.tools import FunctionTool\n",
        "\n",
        "# --- Tool Definitions ---\n",
        "# These are simple Python functions that our agent can call.\n",
        "# In a real application, these might connect to a database or a live API.\n",
        "\n",
        "def get_product_details(product_name: str) -> dict:\n",
        "    \"\"\"Gathers basic details about a product.\"\"\"\n",
        "    details = {\n",
        "        \"smartphone\": \"A cutting-edge smartphone with advanced features.\",\n",
        "        \"laptop\": \"A high-performance laptop for work and play.\",\n",
        "        \"headphones\": \"Wireless headphones with noise cancellation.\",\n",
        "    }\n",
        "    return details.get(product_name.lower(), \"Product details not found.\")\n",
        "\n",
        "def get_product_price(product_name: str) -> dict:\n",
        "    \"\"\"Gathers the price of a product.\"\"\"\n",
        "    prices = {\"smartphone\": \"$999\", \"laptop\": \"$1299\", \"headphones\": \"$199\"}\n",
        "    return prices.get(product_name.lower(), \"Product price not found.\")\n",
        "\n",
        "# --- Tool Objects ---\n",
        "# We wrap our functions in the `FunctionTool` class so the Agent can use them.\n",
        "get_product_details_tool = FunctionTool(get_product_details)\n",
        "get_product_price_tool = FunctionTool(get_product_price)\n",
        "\n",
        "# --- Agent Definition ---\n",
        "# These instructions are the agent's guide. It tells the LLM how to behave\n",
        "# and when to use the tools we've provided.\n",
        "AGENT_INSTRUCTIONS = textwrap.dedent(\"\"\"\n",
        "    You are a product research assistant.\n",
        "    - If the user asks for \"price\", \"cost\", or \"how much\", you MUST use the `get_product_price` tool.\n",
        "    - If the user asks for \"details\" or \"about\", you MUST use the `get_product_details` tool.\n",
        "    - If a query mentions both price and details, you MUST prioritize the `get_product_price` tool.\n",
        "    - Respond only with the direct output from the tool.\n",
        "\"\"\")\n",
        "\n",
        "# We create the agent, providing the model, instructions, and tools.\n",
        "root_agent = LlmAgent(\n",
        "    name=\"ProductAgent\",\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    instruction=AGENT_INSTRUCTIONS,\n",
        "    tools=[get_product_details_tool, get_product_price_tool],\n",
        ")"
      ],
      "metadata": {
        "id": "xEOmcJsISmOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Unit Testing: Verifying Your Tools\n",
        "\n",
        "Before we test the whole agent, it's a best practice to test its individual components. This is called **Unit Testing**. If the tools don't work, the agent can't work!\n",
        "\n",
        "We use Python's built-in `unittest` framework to write a few simple checks:\n",
        "- We test that our functions return the correct data when a product is found (e.g., `get_product_price(\"smartphone\")` should be `\"$999\"`).\n",
        "- We test the \"unhappy path\": what happens when a product is *not* found (e.g., `get_product_price(\"toaster\")` should return the \"not found\" message).\n",
        "- We also check that the functions are case-insensitive, as we designed them to be.\n",
        "\n",
        "Seeing `OK` in the output means all our tool functions are behaving exactly as we expect. ‚úÖ"
      ],
      "metadata": {
        "id": "unit_test_explanation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Run Unit Tests for Tools\n",
        "import unittest\n",
        "# We import the functions directly from the agent.py file we just created.\n",
        "from agent import get_product_price, get_product_details\n",
        "\n",
        "class TestProductTools(unittest.TestCase):\n",
        "    \"\"\"A test suite for the agent's individual tools.\"\"\"\n",
        "    def test_get_price_found(self):\n",
        "        self.assertEqual(get_product_price(\"smartphone\"), \"$999\")\n",
        "        self.assertEqual(get_product_price(\"LAPTOP\"), \"$1299\")\n",
        "\n",
        "    def test_get_price_not_found(self):\n",
        "        self.assertEqual(get_product_price(\"toaster\"), \"Product price not found.\")\n",
        "\n",
        "    def test_get_details_found(self):\n",
        "        self.assertEqual(get_product_details(\"headphones\"), \"Wireless headphones with noise cancellation.\")\n",
        "\n",
        "    def test_get_details_not_found(self):\n",
        "        self.assertEqual(get_product_details(\"watch\"), \"Product details not found.\")\n",
        "\n",
        "print(\"--- Running Tool Unit Tests ---\")\n",
        "# This command runs the tests defined in the class above.\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2)"
      ],
      "metadata": {
        "id": "KvA9OoSnVFYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Evaluation: Creating Test Cases for the Agent\n",
        "\n",
        "Now for the exciting part: testing the agent's reasoning. While unit tests check if a function works, **evaluation** checks if the agent *chooses the right function* and produces the correct final answer.\n",
        "\n",
        "We create a JSON file (`evaluation.test.json`) to define our test cases. This is a standard format for agent evaluation. Each `eval_case` has:\n",
        "- **`user_content`**: The question we will ask the agent.\n",
        "- **`final_response`**: The *exact* answer we expect the agent to give.\n",
        "\n",
        "This file acts as our \"answer key.\" We'll use it in the next step to automatically grade our agent's performance."
      ],
      "metadata": {
        "id": "eval_json_explanation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Create Agent Evaluation Set (JSON)\n",
        "%%writefile evaluation.test.json\n",
        "{\n",
        "  \"eval_set_id\": \"product_agent_eval_set\",\n",
        "  \"eval_cases\": [\n",
        "    {\n",
        "      \"eval_id\": \"get_smartphone_price\",\n",
        "      \"conversation\": [\n",
        "        {\n",
        "          \"user_content\": { \"parts\": [{ \"text\": \"How much does the smartphone cost?\" }] },\n",
        "          \"final_response\": { \"parts\": [{ \"text\": \"$999\" }] }\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"eval_id\": \"get_laptop_details\",\n",
        "      \"conversation\": [\n",
        "        {\n",
        "          \"user_content\": { \"parts\": [{ \"text\": \"Tell me about the laptop\" }] },\n",
        "          \"final_response\": { \"parts\": [{ \"text\": \"A high-performance laptop for work and play.\" }] }\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"eval_id\": \"get_unknown_product_price\",\n",
        "      \"conversation\": [\n",
        "        {\n",
        "          \"user_content\": { \"parts\": [{ \"text\": \"What is the price of a toaster?\" }] },\n",
        "          \"final_response\": { \"parts\": [{ \"text\": \"Product price not found.\" }] }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "_xjuWIzDSqno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Execution: Running the Evaluation\n",
        "\n",
        "This final code block brings everything together. It runs our evaluation by systematically comparing the agent's performance against the \"answer key\" we created in `evaluation.test.json`.\n",
        "\n",
        "Here‚Äôs the process:\n",
        "1.  **Load Data**: It opens and reads the `evaluation.test.json` file.\n",
        "2.  **Setup Runner**: It uses the `Runner` from the ADK, which is the engine for executing agent interactions.\n",
        "3.  **Loop Through Cases**: The code iterates through each test case from the JSON file.\n",
        "4.  **Run Query**: For each case, it sends the `user_content` (the query) to our agent.\n",
        "5.  **Compare Results**: It takes the agent's final text response (`actual`) and compares it directly to the `final_response` from our file (`expected`).\n",
        "6.  **Display Report**: Finally, it prints a clean report showing which tests `PASSED` and which `FAILED`.\n",
        "\n",
        "If all tests pass, congratulations! üéâ You've successfully built, tested, and evaluated your first AI agent."
      ],
      "metadata": {
        "id": "eval_runner_explanation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Run the Agent Evaluation\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "from google.adk import Runner\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.genai.types import Content, Part\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Import the agent we defined in agent.py\n",
        "from agent import root_agent\n",
        "\n",
        "# --- Setup Dependencies ---\n",
        "# The evaluation framework needs a session service to manage conversations.\n",
        "# InMemorySessionService is a simple one for local testing.\n",
        "session_service = InMemorySessionService()\n",
        "my_user_id = f\"user-{uuid.uuid4()}\" # A unique ID for the user\n",
        "\n",
        "def result(input: str)-> str:\n",
        "  \"\"\"\n",
        "  Check if a string matches the format using a regex.\n",
        "  \"\"\"\n",
        "  pattern = r'^\\{\\s*\"[^\"]+\"\\s*:\\s*\\{\\s*\"result\"\\s*:\\s*\"[^\"]*\"\\s*\\}\\s*\\}$'\n",
        "  print(f\"{input}\")\n",
        "  if re.match(pattern, input):\n",
        "    start_result = '\"result\": \"'\n",
        "    start_index = input.find(start_result)\n",
        "    valore_start_index = start_index + len(start_result)\n",
        "    end_index = input.find('\"', valore_start_index)\n",
        "    input=input[valore_start_index:end_index]\n",
        "  return input\n",
        "\n",
        "# --- Helper function to run a single query ---\n",
        "async def run_agent_query(agent, query: str, session, user_id: str):\n",
        "    runner = Runner(agent=agent, session_service=session_service, app_name=agent.name)\n",
        "    final_response = \"\"\n",
        "    try:\n",
        "        # Run the agent and wait for events\n",
        "        async for event in runner.run_async(\n",
        "            user_id=user_id,\n",
        "            session_id=session.id,\n",
        "            new_message=Content(parts=[Part(text=query)], role=\"user\")\n",
        "        ):\n",
        "            # We only care about the final text response for this test\n",
        "            if event.is_final_response():\n",
        "                final_response = event.content.parts[0].text\n",
        "    except Exception as e:\n",
        "        final_response = f\"An error occurred: {e}\"\n",
        "    return final_response\n",
        "\n",
        "# --- Main Evaluation Loop ---\n",
        "async def run_agent_evaluation(agent, eval_cases: list, user_id: str):\n",
        "    results = []\n",
        "    print(f\"--- üß™ Starting Evaluation for Agent: {agent.name} ---\")\n",
        "    for case in eval_cases:\n",
        "        query = case[\"conversation\"][0][\"user_content\"][\"parts\"][0][\"text\"]\n",
        "        expected = case[\"conversation\"][0][\"final_response\"][\"parts\"][0][\"text\"]\n",
        "        eval_id = case[\"eval_id\"]\n",
        "\n",
        "        print(f\"\\nRunning Case: '{eval_id}'...\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "\n",
        "        # Create a new session for each evaluation case to ensure they are independent\n",
        "        session = await session_service.create_session(app_name=agent.name, user_id=user_id)\n",
        "        actual = await run_agent_query(agent, query, session, user_id)\n",
        "\n",
        "        # The core of the evaluation: is the actual response what we expected?\n",
        "        actual=result(actual)\n",
        "        passed = (expected.strip() == actual.strip())\n",
        "\n",
        "        results.append({\n",
        "            \"eval_id\": eval_id,\n",
        "            \"passed\": passed,\n",
        "            \"expected\": expected,\n",
        "            \"actual\": actual\n",
        "        })\n",
        "\n",
        "    print(\"\\n--- ‚úÖ Evaluation Complete ---\")\n",
        "    return results\n",
        "\n",
        "# --- Execute the Evaluation ---\n",
        "with open(\"evaluation.test.json\") as f:\n",
        "    eval_data = json.load(f)\n",
        "\n",
        "results = await run_agent_evaluation(root_agent, eval_data[\"eval_cases\"], user_id=my_user_id)\n",
        "\n",
        "# --- Display the Results ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"          EVALUATION RESULTS\")\n",
        "print(\"=\"*40)\n",
        "for result in results:\n",
        "    status = \"‚úÖ PASSED\" if result[\"passed\"] else \"‚ùå FAILED\"\n",
        "    print(f\"\\nüß™ Eval ID:  {result['eval_id']} ({status})\")\n",
        "    print(f\"  - Expected: {result['expected']}\")\n",
        "    print(f\"  - Actual:   {result['actual']}\")\n",
        "    print(\"‚Äî\" * 40)"
      ],
      "metadata": {
        "id": "NvjCgGuYSsgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}